{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f12605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import unsloth\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58dc2d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PyTorch can access the GPU.\n",
      "   - GPU Device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\" PyTorch can access the GPU.\")\n",
    "    print(f\"   - GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\" PyTorch cannot access the GPU. Please check your CUDA installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf121ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PyTorch version:  2.7.1+cu126\n",
      "âœ… Unsloth version:  2025.7.7\n",
      "âœ… Bitsandbytes version:  0.46.1\n",
      "\n",
      "Your environment is ready for GPU fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "print(f\"âœ… PyTorch version:  {torch.__version__}\")\n",
    "print(f\"âœ… Unsloth version:  {unsloth.__version__}\")\n",
    "print(f\"âœ… Bitsandbytes version:  {bitsandbytes.__version__}\")\n",
    "print(\"\\nYour environment is ready for GPU fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd167e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:47:34,605 - INFO - Saving all datasets to ./gemma_finetune_datasets...\n",
      "2025-07-23 17:47:34,606 - INFO - Creating and saving 'phase1'...\n",
      "2025-07-23 17:47:34,606 - INFO - Building dataset for General & Emotion phase...\n",
      "2025-07-23 17:47:34,607 - INFO - Loading and preparing all source datasets...\n",
      "2025-07-23 17:47:34,607 - INFO - Processing source: emotion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Phase Information\n",
      "============================================================\n",
      "\n",
      "PHASE1 (General & Emotion)\n",
      "  Purpose: Basic conversational and emotional understanding\n",
      "  Sources: openorca, ultrachat, emotion\n",
      "\n",
      "PHASE2 (Therapeutic)\n",
      "  Purpose: Empathetic counseling and therapeutic dialogue\n",
      "  Sources: empathetic_dialogues, mentalchat16k, counsel_chat\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5e5e80a88f427182a059a806489922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:47:43,823 - INFO - Processing source: ultrachat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b20353d74e2402394edd37181c6f4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644db5f0d0104185beaf7ef12a6d06fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cc55a18923460484b069191f966df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/24998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:47:59,686 - INFO - Processing source: openorca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8b06d06249478c871b1dadbc2e87a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:48:08,058 - INFO - Processing source: counsel_chat\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "2025-07-23 17:48:09,521 - WARNING - Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3fe855049841ac85744b8325c0e40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:48:15,240 - INFO - Processing source: mentalchat16k\n",
      "2025-07-23 17:48:18,291 - INFO - Processing source: empathetic_dialogues\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a0ff6ec2ac4350985d4f145eaeecc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:48:24,728 - INFO - Combining all datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14859fe841a460793665585e4bf88a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/97610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf382cfa7ff841ddaf85189ee695ed04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/79998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:48:34,941 - INFO - Filtering dataset of size 79998 to keep samples under 4096 tokens...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60eb5da9b0024fe8998605ec93a86571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/79998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:48:56,747 - INFO - Removed 115 samples exceeding max token length.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1530cc765ab648208ca6c89c718056e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/71894 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fe1e01c47741e081ce5a7cce347b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:48:59,012 - INFO - Saved 'phase1' with 71894 train and 7989 validation samples.\n",
      "2025-07-23 17:48:59,012 - INFO - Creating and saving 'phase2'...\n",
      "2025-07-23 17:48:59,012 - INFO - Building dataset for Therapeutic phase...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d95693e6ae44ba9c55ad21c34f3b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/97610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9facc701c2894e4ca1eca96180085740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/17612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:49:06,271 - INFO - Filtering dataset of size 17612 to keep samples under 4096 tokens...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9057bdcd6e43fe8ea8ecb33a7441d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/17612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:49:11,340 - INFO - Removed 7 samples exceeding max token length.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2f8aab4f10429d97805ba8fa54b181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a7ed2d524d44ed83577f215b387f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:49:11,735 - INFO - Saved 'phase2' with 15844 train and 1761 validation samples.\n",
      "2025-07-23 17:49:11,736 - INFO - Creating and saving 'complete' dataset...\n",
      "2025-07-23 17:49:11,737 - INFO - Building complete dataset with domain tags...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b69c32dfc7c439f8108a1cdf8765fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/97610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:49:20,195 - INFO - Filtering dataset of size 97610 to keep samples under 4096 tokens...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c1392cae87446c8abcaee81f7a4a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/97610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:49:43,740 - INFO - Removed 122 samples exceeding max token length.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dfa131682241a68a0ca924b2b69563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/87739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b8bfae4b034785a12df92aa7a732db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 17:49:46,506 - INFO - Saved 'complete' with 87739 train and 9749 validation samples.\n",
      "2025-07-23 17:49:46,507 - INFO - All datasets saved successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Stage-wise Training Commands\n",
      "============================================================\n",
      "\n",
      "# Phase 1 (General & Emotion)\n",
      "python train_sft.py --data_path ./gemma_finetune_datasets/phase1 --output_dir ./checkpoints/phase1 --epochs 1\n",
      "\n",
      "# Phase 2 (Therapeutic)\n",
      "python train_sft.py --data_path ./gemma_finetune_datasets/phase2 --resume_from ./checkpoints/phase1 --output_dir ./checkpoints/phase2 --epochs 1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class GemmaDatasetBuilder:\n",
    "    \"\"\"\n",
    "    Professional dataset builder for Gemma 3 4B fine-tuning.\n",
    "    Uses verified, large-scale datasets for general chat, emotion,\n",
    "    and therapeutic dialogue.\n",
    "    \n",
    "    Features:\n",
    "    - Domain tag injection for style switching\n",
    "    - Stage-wise curriculum support\n",
    "    \"\"\"\n",
    "\n",
    "    SOURCES: Dict[str, str] = {\n",
    "        # General instruction-following\n",
    "        \"openorca\": \"Open-Orca/OpenOrca\",\n",
    "        # Open-domain multi-turn chat\n",
    "        \"ultrachat\": \"HuggingFaceH4/ultrachat_200k\",\n",
    "        # Safety/preference alignment\n",
    "        \"ultrafeedback\": \"HuggingFaceH4/ultrafeedback_binarized\",\n",
    "        # Emotion understanding\n",
    "        \"emotion\": \"dair-ai/emotion\",\n",
    "        # Empathetic multi-turn counseling\n",
    "        \"empathetic_dialogues\": \"facebook/empathetic_dialogues\",\n",
    "        # Benchmark mental-health counseling\n",
    "        \"mentalchat16k\": \"ShenLab/MentalChat16K\",\n",
    "        # Real counselor Q&A\n",
    "        \"counsel_chat\": \"nbertagnolli/counsel-chat\"\n",
    "    }\n",
    "\n",
    "    # Define training phases for curriculum learning\n",
    "    TRAINING_PHASES = {\n",
    "        \"phase1\": {\n",
    "            \"name\": \"General & Emotion\",\n",
    "            \"sources\": [\"openorca\", \"ultrachat\", \"emotion\"],\n",
    "            \"description\": \"Basic conversational and emotional understanding\"\n",
    "        },\n",
    "        \"phase2\": {\n",
    "            \"name\": \"Therapeutic\",\n",
    "            \"sources\": [\"empathetic_dialogues\", \"mentalchat16k\", \"counsel_chat\"],\n",
    "            \"description\": \"Empathetic counseling and therapeutic dialogue\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_tokens: int = 4096,\n",
    "        train_split: float = 0.9,\n",
    "        seed: int = 42,\n",
    "        enable_domain_tags: bool = True\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.train_split = train_split\n",
    "        self.seed = seed\n",
    "        self.enable_domain_tags = enable_domain_tags\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "        self.logger = self._setup_logging()\n",
    "        self._prepared_data: Optional[Dataset] = None\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        return logging.getLogger(__name__)\n",
    "\n",
    "    def _tagged(self, text: str, domain: str) -> str:\n",
    "        \"\"\"Prepend a domain tag to any instruction for style switching.\"\"\"\n",
    "        if self.enable_domain_tags:\n",
    "            return f\"<domain>{domain}</domain>\\n{text}\"\n",
    "        return text\n",
    "\n",
    "    def _load_and_prepare_sources(self) -> Dataset:\n",
    "        \"\"\"\n",
    "        Loads all unique datasets, processes them into a unified format,\n",
    "        and concatenates them. This is the efficient core of the builder.\n",
    "        \"\"\"\n",
    "        if self._prepared_data is not None:\n",
    "            return self._prepared_data\n",
    "\n",
    "        self.logger.info(\"Loading and preparing all source datasets...\")\n",
    "        \n",
    "        source_map = {\n",
    "            \"openorca\": self._load_openorca,\n",
    "            \"ultrachat\": self._load_ultrachat,\n",
    "            \"ultrafeedback\": self._load_ultrafeedback,\n",
    "            \"emotion\": self._load_emotion,\n",
    "            \"empathetic_dialogues\": self._load_empathetic,\n",
    "            \"mentalchat16k\": self._load_mentalchat,\n",
    "            \"counsel_chat\": self._load_counsel\n",
    "        }\n",
    "\n",
    "        all_sources = set(s for phase in self.TRAINING_PHASES.values() for s in phase[\"sources\"])\n",
    "        \n",
    "        parts: List[Dataset] = []\n",
    "        for source_name in all_sources:\n",
    "            if source_name in source_map:\n",
    "                try:\n",
    "                    self.logger.info(f\"Processing source: {source_name}\")\n",
    "                    dataset = source_map[source_name]()\n",
    "                    if dataset:\n",
    "                        parts.append(dataset)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Failed to load or process source {source_name}: {e}\", exc_info=True)\n",
    "            else:\n",
    "                self.logger.warning(f\"No loader found for source: {source_name}\")\n",
    "\n",
    "        if not parts:\n",
    "            raise ValueError(\"No datasets could be loaded. Aborting.\")\n",
    "\n",
    "        self.logger.info(\"Combining all datasets...\")\n",
    "        combined = concatenate_datasets(parts).shuffle(seed=self.seed)\n",
    "        self._prepared_data = combined\n",
    "        return self._prepared_data\n",
    "\n",
    "    def _format_conversation(self, ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        ins = ex[\"instruction\"].strip()\n",
    "        rsp = ex[\"response\"].strip()\n",
    "        \n",
    "        return {\n",
    "            \"text\": f\"<start_of_turn>user\\n{ins}<end_of_turn>\\n<start_of_turn>model\\n{rsp}<end_of_turn>\",\n",
    "            \"source\": ex.get(\"source\"), \n",
    "            \"domain\": ex.get(\"domain\")\n",
    "        }\n",
    "\n",
    "    def _filter_and_format_dataset(self, dataset: Dataset) -> Dataset:\n",
    "        \"\"\"Filters out long sequences and formats the text.\"\"\"\n",
    "        \n",
    "        # 1. Format the text first\n",
    "        formatted_ds = dataset.map(self._format_conversation, num_proc=4)\n",
    "        \n",
    "        # 2. Filter out samples that are too long\n",
    "        original_size = len(formatted_ds)\n",
    "        self.logger.info(f\"Filtering dataset of size {original_size} to keep samples under {self.max_tokens} tokens...\")\n",
    "        \n",
    "        filtered_ds = formatted_ds.filter(\n",
    "            lambda x: len(self.tokenizer.encode(x[\"text\"])) <= self.max_tokens,\n",
    "            num_proc=4\n",
    "        )\n",
    "        \n",
    "        new_size = len(filtered_ds)\n",
    "        if original_size > new_size:\n",
    "            self.logger.info(f\"Removed {original_size - new_size} samples exceeding max token length.\")\n",
    "            \n",
    "        return filtered_ds\n",
    "\n",
    "    def build_complete_dataset(self) -> DatasetDict:\n",
    "        \"\"\"Load, combine, format, filter, and split all verified datasets.\"\"\"\n",
    "        self.logger.info(\"Building complete dataset with domain tags...\")\n",
    "        \n",
    "        prepared_data = self._load_and_prepare_sources()\n",
    "        final_data = self._filter_and_format_dataset(prepared_data)\n",
    "\n",
    "        total = len(final_data)\n",
    "        if total == 0:\n",
    "            raise ValueError(\"The dataset is empty after filtering. Check max_tokens or source data.\")\n",
    "            \n",
    "        train_size = int(total * self.train_split)\n",
    "        \n",
    "        return DatasetDict(\n",
    "            train=final_data.select(range(train_size)),\n",
    "            validation=final_data.select(range(train_size, total))\n",
    "        )\n",
    "\n",
    "    def build_phase_dataset(self, phase: str) -> DatasetDict:\n",
    "        \"\"\"Build dataset for a specific training phase using pre-loaded data.\"\"\"\n",
    "        if phase not in self.TRAINING_PHASES:\n",
    "            raise ValueError(f\"Unknown phase: {phase}. Available: {list(self.TRAINING_PHASES.keys())}\")\n",
    "        \n",
    "        phase_info = self.TRAINING_PHASES[phase]\n",
    "        self.logger.info(f\"Building dataset for {phase_info['name']} phase...\")\n",
    "        \n",
    "        prepared_data = self._load_and_prepare_sources()\n",
    "        \n",
    "        phase_sources = phase_info[\"sources\"]\n",
    "        phase_data = prepared_data.filter(lambda x: x['source'] in phase_sources, num_proc=4)\n",
    "        \n",
    "        if len(phase_data) == 0:\n",
    "            raise ValueError(f\"No data found for sources in phase {phase}: {phase_sources}\")\n",
    "\n",
    "        final_data = self._filter_and_format_dataset(phase_data)\n",
    "\n",
    "        total = len(final_data)\n",
    "        if total == 0:\n",
    "            raise ValueError(f\"The dataset for phase '{phase}' is empty after filtering.\")\n",
    "\n",
    "        train_size = int(total * self.train_split)\n",
    "        \n",
    "        return DatasetDict(\n",
    "            train=final_data.select(range(train_size)),\n",
    "            validation=final_data.select(range(train_size, total))\n",
    "        )\n",
    "\n",
    "    def _load_openorca(self) -> Dataset:\n",
    "        ds = load_dataset(self.SOURCES[\"openorca\"], split=\"train[:50000]\")\n",
    "        ds = ds.filter(lambda ex: ex.get(\"question\") and ex.get(\"response\"))\n",
    "        return ds.map(lambda ex: {\n",
    "            \"instruction\": self._tagged(ex[\"question\"].strip(), \"general\"),\n",
    "            \"response\": ex[\"response\"].strip(),\n",
    "            \"source\": \"openorca\",\n",
    "            \"domain\": \"general\"\n",
    "        }, num_proc=4)\n",
    "\n",
    "    def _load_ultrachat(self) -> Dataset:\n",
    "        ds = load_dataset(self.SOURCES[\"ultrachat\"], split=\"train_sft[:25000]\")\n",
    "        def extract(item):\n",
    "            msgs = item.get(\"messages\", [])\n",
    "            if len(msgs) >= 2 and msgs[0].get(\"content\") and msgs[1].get(\"content\"):\n",
    "                return {\"instruction\": self._tagged(msgs[0][\"content\"].strip(), \"chat\"), \"response\": msgs[1][\"content\"].strip()}\n",
    "            return {\"instruction\": None, \"response\": None}\n",
    "        \n",
    "        ds = ds.map(extract, num_proc=4).filter(lambda x: x[\"instruction\"] is not None)\n",
    "        return ds.add_column(\"source\", [\"ultrachat\"] * len(ds)).add_column(\"domain\", [\"chat\"] * len(ds))\n",
    "\n",
    "    def _load_ultrafeedback(self) -> Dataset:\n",
    "        ds = load_dataset(self.SOURCES[\"ultrafeedback\"], split=\"train_prefs[:10000]\")\n",
    "        def extract(item):\n",
    "            prompt = item.get(\"prompt\", \"\").strip()\n",
    "            chosen = item.get(\"chosen\", [])\n",
    "            if prompt and chosen:\n",
    "                response_content = (chosen[-1].get(\"content\") if isinstance(chosen[-1], dict) else str(chosen[-1]))\n",
    "                if response_content:\n",
    "                    return {\"instruction\": self._tagged(prompt, \"safety\"), \"response\": response_content.strip()}\n",
    "            return {\"instruction\": None, \"response\": None}\n",
    "\n",
    "        ds = ds.map(extract, num_proc=4).filter(lambda x: x[\"instruction\"] is not None)\n",
    "        return ds.add_column(\"source\", [\"ultrafeedback\"] * len(ds)).add_column(\"domain\", [\"safety\"] * len(ds))\n",
    "\n",
    "    def _load_emotion(self) -> Dataset:\n",
    "        ds = load_dataset(self.SOURCES[\"emotion\"], split=\"train[:5000]\")\n",
    "        ds = ds.filter(lambda ex: ex.get(\"text\") is not None)\n",
    "        labels = ds.features['label'].names\n",
    "        def extract(ex):\n",
    "            txt = ex[\"text\"].strip()\n",
    "            emo = labels[ex[\"label\"]]\n",
    "            ins = f\"What emotion is expressed in the following text?\\n'{txt}'\"\n",
    "            rsp = f\"The primary emotion expressed is {emo}.\"\n",
    "            return {\"instruction\": self._tagged(ins, \"emotion\"), \"response\": rsp}\n",
    "        ds = ds.map(extract, num_proc=4, remove_columns=ds.column_names)\n",
    "        return ds.add_column(\"source\", [\"emotion\"] * len(ds)).add_column(\"domain\", [\"emotion\"] * len(ds))\n",
    "\n",
    "    def _load_empathetic(self) -> Dataset:\n",
    "        ds = load_dataset(self.SOURCES[\"empathetic_dialogues\"], split=\"train[:15000]\")\n",
    "        ds = ds.filter(lambda ex: ex.get(\"prompt\") and ex.get(\"utterance\"))\n",
    "        ds = ds.map(lambda ex: {\n",
    "            \"instruction\": self._tagged(ex[\"prompt\"].strip(), \"therapeutic\"), \n",
    "            \"response\": ex[\"utterance\"].strip(), \n",
    "        }, num_proc=4, remove_columns=ds.column_names)\n",
    "        return ds.add_column(\"source\", [\"empathetic_dialogues\"] * len(ds)).add_column(\"domain\", [\"therapeutic\"] * len(ds))\n",
    "\n",
    "    def _load_mentalchat(self) -> Dataset:\n",
    "        ds = load_dataset(self.SOURCES[\"mentalchat16k\"], split=\"train[:16000]\")\n",
    "        ds = ds.filter(lambda ex: ex.get(\"instruction\") and ex.get(\"chosen\"))\n",
    "        ds = ds.map(lambda ex: {\n",
    "            \"instruction\": self._tagged(ex[\"instruction\"].strip(), \"therapeutic\"),\n",
    "            \"response\": (ex[\"chosen\"].strip() if isinstance(ex[\"chosen\"], str) else ex[\"chosen\"][0].strip()),\n",
    "        }, num_proc=4, remove_columns=ds.column_names)\n",
    "        return ds.add_column(\"source\", [\"mentalchat16k\"] * len(ds)).add_column(\"domain\", [\"therapeutic\"] * len(ds))\n",
    "\n",
    "    def _load_counsel(self) -> Dataset:\n",
    "        ds = load_dataset(self.SOURCES[\"counsel_chat\"], split=\"train\")\n",
    "        ds = ds.filter(lambda ex: ex.get(\"questionText\") and ex.get(\"answerText\"))\n",
    "        ds = ds.map(lambda ex: {\n",
    "            \"instruction\": self._tagged(ex[\"questionText\"].strip(), \"therapeutic\"),\n",
    "            \"response\": ex[\"answerText\"].strip(),\n",
    "        }, num_proc=4, remove_columns=ds.column_names)\n",
    "        return ds.add_column(\"source\", [\"counsel_chat\"] * len(ds)).add_column(\"domain\", [\"therapeutic\"] * len(ds))\n",
    "\n",
    "    def save_phase_datasets(self, output_dir: str = \"./phase_datasets\"):\n",
    "        \"\"\"Build and save individual phase datasets and the complete dataset.\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        self.logger.info(f\"Saving all datasets to {output_dir}...\")\n",
    "        \n",
    "        for phase_name in self.TRAINING_PHASES:\n",
    "            self.logger.info(f\"Creating and saving '{phase_name}'...\")\n",
    "            phase_dataset = self.build_phase_dataset(phase_name)\n",
    "            phase_path = output_path / phase_name\n",
    "            phase_dataset.save_to_disk(str(phase_path))\n",
    "            self.logger.info(f\"Saved '{phase_name}' with {len(phase_dataset['train'])} train and {len(phase_dataset['validation'])} validation samples.\")\n",
    "        \n",
    "        self.logger.info(\"Creating and saving 'complete' dataset...\")\n",
    "        complete_dataset = self.build_complete_dataset()\n",
    "        complete_path = output_path / \"complete\"\n",
    "        complete_dataset.save_to_disk(str(complete_path))\n",
    "        self.logger.info(f\"Saved 'complete' with {len(complete_dataset['train'])} train and {len(complete_dataset['validation'])} validation samples.\")\n",
    "        \n",
    "        self.logger.info(\"All datasets saved successfully!\")\n",
    "\n",
    "    def print_phase_info(self):\n",
    "        \"\"\"Print information about training phases.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Training Phase Information\")\n",
    "        print(\"=\" * 60)\n",
    "        for phase_name, phase_info in self.TRAINING_PHASES.items():\n",
    "            print(f\"\\n{phase_name.upper()} ({phase_info['name']})\")\n",
    "            print(f\"  Purpose: {phase_info['description']}\")\n",
    "            print(f\"  Sources: {', '.join(phase_info['sources'])}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "def get_training_commands(base_dir: str = \"./phase_datasets\", output_dir: str = \"./checkpoints\"):\n",
    "    \"\"\"Return the training commands for each phase.\"\"\"\n",
    "    commands = {\n",
    "        \"Phase 1 (General & Emotion)\":\n",
    "            f\"python train_sft.py --data_path {base_dir}/phase1 --output_dir {output_dir}/phase1 --epochs 1\",\n",
    "        \"Phase 2 (Therapeutic)\":\n",
    "            f\"python train_sft.py --data_path {base_dir}/phase2 --resume_from {output_dir}/phase1 --output_dir {output_dir}/phase2 --epochs 1\"\n",
    "    }\n",
    "    return commands\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the dataset building process.\n",
    "    \"\"\"\n",
    "    builder = GemmaDatasetBuilder(enable_domain_tags=True)\n",
    "    builder.print_phase_info()\n",
    "    builder.save_phase_datasets(\"./gemma_finetune_datasets\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Stage-wise Training Commands\")\n",
    "    print(\"=\" * 60)\n",
    "    commands = get_training_commands(\"./gemma_finetune_datasets\")\n",
    "    for phase_name, cmd in commands.items():\n",
    "        print(f\"\\n# {phase_name}\")\n",
    "        print(cmd)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ff138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71cfb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Using dtype: torch.bfloat16\n",
      "==((====))==  Unsloth 2025.7.7: Fast Gemma3 patching. Transformers: 4.53.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3070 Laptop GPU. Num GPUs = 1. Max memory: 7.632 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "model.safetensors:   2%|â–Ž                  | 71.9M/4.56G [00:38<22:38, 3.31MB/s]^C\n",
      "Cancellation requested; stopping current tasks.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 627, in xet_get\n",
      "    download_files(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sanj-ai/Gemma_4b_Finetuning/run_sft.py\", line 157, in <module>\n",
      "    main()\n",
      "  File \"/home/sanj-ai/Gemma_4b_Finetuning/run_sft.py\", line 77, in main\n",
      "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/loader.py\", line 339, in from_pretrained\n",
      "    return FastModel.from_pretrained(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/loader.py\", line 797, in from_pretrained\n",
      "    model, tokenizer = FastBaseModel.from_pretrained(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/vision.py\", line 424, in from_pretrained\n",
      "    model = auto_model.from_pretrained(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 600, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 311, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4680, in from_pretrained\n",
      "    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1137, in _get_resolved_checkpoint_files\n",
      "    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 312, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/hub.py\", line 470, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1008, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1161, in _hf_hub_download_to_cache_dir\n",
      "    _download_to_tmp_and_move(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1710, in _download_to_tmp_and_move\n",
      "    xet_get(\n",
      "  File \"/home/sanj-ai/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 627, in xet_get\n",
      "    download_files(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python run_sft.py \\\n",
    "  --model_name \"google/gemma-3-4b-it\" \\\n",
    "  --data_path ./gemma_finetune_datasets/phase1 \\\n",
    "  --output_dir ./checkpoints/phase1 \\\n",
    "  --epochs 1 \\\n",
    "  --batch_size 2 \\\n",
    "  --grad_accum 4 \\\n",
    "  --report_to \"wandb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27075a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name MODEL_NAME] --data_path\n",
      "                             DATA_PATH --output_dir OUTPUT_DIR\n",
      "                             [--resume_from RESUME_FROM] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--grad_accum GRAD_ACCUM] [--lr LR]\n",
      "                             [--max_len MAX_LEN] [--seed SEED]\n",
      "                             [--report_to REPORT_TO]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_path, --output_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
